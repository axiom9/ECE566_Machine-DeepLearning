{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y1LJdNtE3Fqn"},"source":["## Load packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"6Gqh8kPlgiFJ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5hWACQotBtYJ"},"source":["## Define functions"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j0Lx7rAg3L4o"},"source":["### Sigmoid"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"bm7_VffO3CFb"},"outputs":[],"source":["def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(z)\n","    \"\"\"\n","\n","    s = (1) / (1+np.exp(-x))\n","    \n","    return s"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S9Jv5CfG3LQk"},"source":["### Initialize weights"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"StsRU276CPv4"},"outputs":[],"source":["def initialize_weights(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","    \n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","    \n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","    w = np.zeros(shape=(dim, 1))\n","    b = 0\n","  \n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mY_hYmALCSW2"},"source":["### Forward and backward propagation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"_YDXRCMoCW18"},"outputs":[],"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained in the assignment\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of size (number of examples, num_px * num_px)\n","    Y -- true \"label\" vector of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    \"\"\"\n","    \n","    m = X.shape[0]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","   \n","    y_hat = sigmoid(np.dot(w.T, X.T) + b)\n","\n","    #âˆ’ð‘¦(\") ð‘™ð‘œð‘”/ð‘¦\"(\")0 âˆ’ /1 âˆ’ ð‘¦(\")0 ð‘™ð‘œð‘”/1 âˆ’ ð‘¦\"(\")0\n","    cost = -(Y[m]*np.log(y_hat)) - (1-Y[m]*(np.log(1-y_hat)))\n","\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","   \n","    \n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bAalkyubClnB"},"source":["### Gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"_oWQ26xrrPRZ"},"outputs":[],"source":["def gradient_descent(w, b, X, Y, num_iterations, learning_rate):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px, number of examples)\n","    Y -- true \"label\" vector of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    \n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # Cost and gradient calculation\n","      \n","        grads, cost = propagate(w, b, X, Y)\n","      \n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update rule\n","        \n","    ?? Add here ??  \n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","            # Print the cost every 100 training examples\n","            print (\"Cost after iteration %i: %f\" % (i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs\n","  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xp3Zv_BTCrik"},"source":["### Make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"rU5EFchuCtX_"},"outputs":[],"source":["def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px, number of examples)\n","    \n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    \n","    m = X.shape[0]\n","    Y_prediction = np.zeros((1, m))\n","    w = w.reshape(X.shape[1], 1)\n","    \n","    # Compute vector \"A\" predicting the probabilities of the picture containing a 1\n","    \n","    A = sigmoid(np.dot(w.T, X.T) + b)\n","    \n","    \n","    for i in range(A.shape[1]):\n","        # Convert probabilities A[0,i] to actual predictions p[0,i]\n","        \n","    ?? Add here ??  \n","\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OaWaPhn0C37b"},"source":["## Merge functions and run your model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"YwuPhKu9gr3C"},"outputs":[],"source":["# LOAD DATA\n","class0 = 0\n","class1 = 1\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train[np.isin(y_train,[class0,class1]),:,:]\n","y_train = 1*(y_train[np.isin(y_train,[class0,class1])]>class0)\n","x_test = x_test[np.isin(y_test,[class0,class1]),:,:]\n","y_test = 1*(y_test[np.isin(y_test,[class0,class1])]>class0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"0EpvnpT4hok1"},"outputs":[],"source":["# RESHAPE\n","\n","x_train_flat = x_train.reshape(x_train.shape[0],-1)\n","print(x_train_flat.shape)\n","print('Train: '+str(x_train_flat.shape[0])+' images and '+str(x_train_flat.shape[1])+' neurons \\n')\n","\n","x_test_flat = x_test.reshape(x_test.shape[0],-1)\n","print(x_test_flat.shape)\n","print('Test: '+str(x_test_flat.shape[0])+' images and '+str(x_test_flat.shape[1])+' neurons \\n')\n","\n","# STRANDARIZE\n","x_train_flat = x_train_flat / 255\n","x_test_flat = x_test_flat / 255"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UU_yVAHXIpbf"},"source":["### Train the model (in training set)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Dfbhuv6Li00c"},"outputs":[],"source":["# Initialize parameters with zeros (â‰ˆ 1 line of code)\n","w, b = initialize_weights(x_train_flat.shape[1])\n","\n","# Gradient descent (â‰ˆ 1 line of code)\n","learning_rate = 0.005\n","num_iterations = 2000\n","parameters, grads, costs = gradient_descent(w, b, x_train_flat, y_train, 2000, 0.005)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qavG0L1TIu5d"},"source":["### Test the model (in testing set)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"zitDghtdIvD1"},"outputs":[],"source":["# Retrieve parameters w and b from dictionary \"parameters\"\n","w = parameters[\"w\"]\n","b = parameters[\"b\"]\n","    \n","# Predict test/train set examples (â‰ˆ 2 lines of code)\n","y_prediction_test = predict(w, b, x_test_flat)\n","y_prediction_train = predict(w, b, x_train_flat)\n","\n","# Print train/test Errors\n","print('')\n","print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n","print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n","print('')\n","\n","plt.figure(figsize=(13,5))\n","plt.plot(range(0,2000,100),costs)\n","plt.title('Cost training vs iteration')\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.xticks(range(0,2000,100))\n","\n","\n","plt.figure(figsize=(13,5))\n","plt.imshow(w.reshape(28,28))\n","plt.title('Template')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Project1_NN_empty.ipynb","provenance":[{"file_id":"1bsY_oxVjvrRmCYQl_64x_7XatcEjnsCh","timestamp":1571339390584},{"file_id":"1Y0bfNpPxW92sJxYZaSwpZDTTO1414tw-","timestamp":1571339345461}]},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"27e2bf90d9e14c46f33b028c0dd6c4e7836b4da87bdde6d6325e44977f74edb2"}}},"nbformat":4,"nbformat_minor":0}
